---
output: 
  github_document:
    toc: false
    fig_width: 10.08
    fig_height: 6
tags: [r, reports]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

# parameters <img src='man/figures/logo.png' align="right" height="139" />

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  tidy.opts=list(width.cutoff=60),
  tidy=TRUE,
  fig.path = "README-"
)
knitr::opts_chunk$set(comment=">")
options(knitr.kable.NA = '',
        digits = 1,
        width=60)

set.seed(333)
```

[![CRAN](http://www.r-pkg.org/badges/version/parameters)](https://cran.r-project.org/package=parameters)
[![downloads](http://cranlogs.r-pkg.org/badges/parameters)](https://cran.r-project.org/package=parameters)
[![Build Status](https://travis-ci.org/easystats/parameters.svg?branch=master)](https://travis-ci.org/easystats/parameters)
[![codecov](https://codecov.io/gh/easystats/parameters/branch/master/graph/badge.svg)](https://codecov.io/gh/easystats/parameters)

***Describe and understand your model's parameters!***

`parameters`'s primary goal is to provide utilities for processing the parameters of various statistical models. Beyond computing *p* values, **CIs**, and other indices for a wide variety of models, this package implements features like **standardization** or **bootstrapping** of parameters and models, **feature reduction** (feature extraction and variable selection) as well as conversion between indices of **effect size**.


## Installation


Run the following:

```{r eval=FALSE, message=FALSE, warning=FALSE}
install.packages("devtools")
devtools::install_github("easystats/parameters")
```
```{r message=FALSE, warning=FALSE}
library("parameters")
```



## Documentation

[![Documentation](https://img.shields.io/badge/documentation-parameters-orange.svg?colorB=E91E63)](https://easystats.github.io/parameters/)
[![Blog](https://img.shields.io/badge/blog-easystats-orange.svg?colorB=FF9800)](https://easystats.github.io/blog/posts/)
[![Features](https://img.shields.io/badge/features-parameters-orange.svg?colorB=2196F3)](https://easystats.github.io/parameters/reference/index.html)

Click on the buttons above to access the package [**documentation**](https://easystats.github.io/parameters/) and the [**easystats blog**](https://easystats.github.io/blog/posts/), and check-out these vignettes:

#### Parameters Engineering

- [Bootstrapped parameters](https://easystats.github.io/parameters/articles/bootstrapping.html)
- [Standardized parameters](https://easystats.github.io/parameters/articles/standardization.html)

#### Variable/Feature Reduction

- [How many factors to retain in Factor Analysis (FA)](https://easystats.github.io/parameters/articles/n_factors.html)
- [Variable extraction (PCA, FA, …)](https://easystats.github.io/parameters/articles/variable_extraction.html)
- [Variable selection (stepwise, projpred, …)](https://easystats.github.io/parameters/articles/variable_selection.html)


# Features
 
## Model's parameters description

<img src='man/figures/figure1.png' align="center" />
 
The `model_parameters` function allows you to extract the parameters and their characteristics from various models in a consistent way. It could be considered as a lightweight alternative to [`broom::tidy()`](https://github.com/tidymodels/broom), with some notable differences: 

- The names of the returned dataframe are **specific** to their content. For instance, the column containing the statistic is named following the statistic name, *i.e.*, *t*, *z*, etc., instead of a generic name such as *statistic*.
- It is able to compute or extract indices not available by default, such as ***p* values**, **CIs**, etc.
- It includes **feature engineering** capabilities, including [**bootstrapping**](https://easystats.github.io/parameters/articles/bootstrapping.html) and [**standardization**](https://easystats.github.io/parameters/articles/standardization.html) of parameters.

### Correlations

#### Frequentist 

```{r, warning=FALSE, message=FALSE, results='hide'}
model <- cor.test(iris$Sepal.Length, iris$Sepal.Width)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```

#### Bayesian 

```{r, warning=FALSE, message=FALSE, results='hide'}
library(BayesFactor)

model <- BayesFactor::correlationBF(iris$Sepal.Length, iris$Sepal.Width)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```


### t-tests

#### Frequentist 
```{r, warning=FALSE, message=FALSE, results='hide'}
df <- iris
df$Sepal.Big <- ifelse(df$Sepal.Width >= 3, "Yes", "No")

model <- t.test(Sepal.Length ~ Sepal.Big, data=df)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```

#### Bayesian 

```{r, warning=FALSE, message=FALSE, results='hide'}
model <- BayesFactor::ttestBF(formula = Sepal.Length ~ Sepal.Big, data=df)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```


### ANOVAs

#### Simple

```{r, warning=FALSE, message=FALSE, results='hide'}
model <- aov(Sepal.Length ~ Sepal.Big, data = df)
model_parameters(model, omega_squared = TRUE)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model, omega_squared = TRUE), digits=1)
```

#### Repeated measures

```{r, warning=FALSE, message=FALSE, results='hide'}
model <- aov(Sepal.Length ~ Sepal.Big + Error(Species), data = df)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```

```{r, warning=FALSE, message=FALSE, results='hide'}
library(lme4)
model <- anova(lmer(Sepal.Length ~ Sepal.Big + (1 | Species), data = df))
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::kable(model_parameters(model), digits=1)
```



### General Linear Models (GLM)
```{r, warning=FALSE, message=FALSE, eval=FALSE}
model <- glm(vs ~ wt + cyl, data = mtcars, family = "binomial")
model_parameters(model, standardize = "refit")
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
model <- glm(vs ~ wt + cyl, data = mtcars, family = "binomial")
knitr::kable(model_parameters(model, standardize = "refit"), digits=1)
```

### Bootstrapped models

```{r, warning=FALSE, message=FALSE, eval=FALSE}
model <- lm(mpg ~ drat * cyl, data = mtcars)
model_parameters(model, bootstrap = TRUE)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
model <- lm(mpg ~ drat * cyl, data = mtcars)
knitr::kable(model_parameters(model, standardize = FALSE, bootstrap = TRUE), digits=1)
```


### Mixed models
```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(lme4)

model <- lmer(Sepal.Width ~ Petal.Length + (1|Species), data = iris)
model_parameters(model, standardize = "refit")
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(lme4)

model <- lmer(Sepal.Width ~ Petal.Length + (1|Species), data = iris)
knitr::kable(model_parameters(model, standardize = "refit"), digits=1)
```



### Bayesian models

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(rstanarm)

model <- stan_glm(mpg ~ wt + cyl, data = mtcars)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(rstanarm)

text <- capture.output(model <- stan_glm(mpg ~ wt + cyl, data = mtcars))
knitr::kable(model_parameters(model), digits=1)
```




### Exploratory Factor Analysis (EFA) and Principal Component Analysis (PCA)

```{r, warning=FALSE, message=FALSE}
library(psych)

model <- psych::fa(attitude, nfactors = 3)
model_parameters(model)
```

### Confirmatory Factor Analysis (CFA) and Structural Equation Models (SEM)

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(lavaan)

model <- lavaan::cfa(' visual  =~ x1 + x2 + x3
                       textual =~ x4 + x5 + x6
                       speed   =~ x7 + x8 + x9 ', 
                     data=HolzingerSwineford1939)
model_parameters(model)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(lavaan)

model <- lavaan::cfa(' visual  =~ x1 + x2 + x3
                       textual =~ x4 + x5 + x6
                       speed   =~ x7 + x8 + x9 ', 
                     data=HolzingerSwineford1939)
knitr::kable(head(model_parameters(model)), digits=1)
```
 
## Variable and parameters selection

<img src='man/figures/figure2.png' align="center" />
 
### General Linear Models (GLM)

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(dplyr)

lm(disp ~ ., data = mtcars) %>% 
  parameters_selection() %>% 
  model_parameters()
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(dplyr)

lm(disp ~ ., data = mtcars) %>% 
  parameters_selection() %>% 
  model_parameters() %>% 
  knitr::kable(digits=1)
```


### Mixed models

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(lme4)

lmer(Sepal.Length ~ Sepal.Width * Petal.Length * Petal.Width + (1|Species), data = iris)  %>%
  parameters_selection() %>%
  model_parameters()
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(lme4)

lmer(Sepal.Length ~ Sepal.Width * Petal.Length * Petal.Width + (1|Species), data = iris)  %>%
  parameters_selection() %>%
  model_parameters() %>%
  knitr::kable(digits=1)
```


### Bayesian models

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(rstanarm)

model <- stan_glm(mpg ~ ., data = mtcars) %>% 
  parameters_selection() %>% 
  model_parameters()
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
text <- capture.output(model <- stan_glm(mpg ~ ., data = mtcars))
parameters_selection(model) %>% 
  model_parameters() %>% 
  knitr::kable(digits=1)
```



 
## Variable and features extraction


### How many factors to retain in Factor Analysis (FA)

```{r, warning=FALSE, message=FALSE}
n_factors(attitude)
```

## Miscellaneous



### Describe a Distribution

```{r, warning=FALSE, message=FALSE}
x <- rnorm(300)
describe_distribution(x)
```


### Standardization and normalization

```{r, warning=FALSE, message=FALSE}
df <- standardize(iris)
summary(df$Sepal.Length)

df <- normalize(iris)
summary(df$Sepal.Length)
```