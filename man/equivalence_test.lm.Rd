% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/equivalence_test.R
\name{equivalence_test.lm}
\alias{equivalence_test.lm}
\alias{equivalence_test.merMod}
\title{Equivalence test}
\usage{
\method{equivalence_test}{lm}(
  x,
  range = "default",
  ci = 0.95,
  p_values = FALSE,
  verbose = TRUE,
  ...
)

\method{equivalence_test}{merMod}(
  x,
  range = "default",
  ci = 0.95,
  effects = c("fixed", "random"),
  p_values = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{x}{A statistical model.}

\item{range}{The range of practical equivalence of an effect. May be \code{"default"},
to automatically define this range based on properties of the model's data.}

\item{ci}{Confidence Interval (CI) level. Default to 0.95 (95\%).}

\item{p_values}{Logical, if \code{TRUE}, adjusted p-values for equivalence testing are calculated.}

\item{verbose}{Toggle off warnings.}

\item{...}{Arguments passed to or from other methods.}

\item{effects}{Should parameters for fixed effects, random effects or both be returned? Only applies to mixed models. May be abbreviated.}
}
\value{
A data frame.
}
\description{
Compute the equivalence test for frequentist models.
}
\details{
The calculation of p-values is somewhat "experimental". For parameters, where H0...
  \itemize{
    \item ... is rejected, the p-value equals a NHST as if the upper / lower boundary of the ROPE (see \code{range}) would be the point-null to test against.
    \item ... is accepted, the p-value is set to 1.
    \item ... is undecided, the p-value equals a NHST against the point-null, however, the "uncertainty" (i.e. ROPE range) is added to the confidence intervals (so the upper confidence interval limit equals the regular upper confidence interval limit + half the ROPE range).
  }
  All p-values are then adjusted for multiple testing (using \code{\link[stats]{p.adjust}} with \code{method = "fdr"}).
}
\examples{
m <- lm(mpg ~ gear + wt + cyl + hp, data = mtcars)
equivalence_test(m)
}
\references{
\itemize{
  \item Campbell, H., & Gustafson, P. (2018). Conditional equivalence testing: An alternative remedy for publication bias. PLOS ONE, 13(4), e0195145. https://doi.org/10.1371/journal.pone.0195145
  \item Kruschke, J. K. (2018). Rejecting or accepting parameter values in Bayesian estimation. Advances in Methods and Practices in Psychological Science, 1(2), 270-280. doi: 10.1177/2515245918771304
  \item Lakens, D. (2017). Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses. Social Psychological and Personality Science, 8(4), 355â€“362. https://doi.org/10.1177/1948550617697177
}
}
\seealso{
For more details, see \code{\link[bayestestR:equivalence_test]{equivalence_test}}.
  Further readings can be found in the references.
}
